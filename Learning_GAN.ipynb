{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Hello All, I am a student currently learning about GAN. I am using this notebook to learn and remember about GAN, you can use this too for your benefit. Iam reffering to the yt video - https://www.youtube.com/watch?v=OljTVUVzPpM&t=189s&ab_channel=AladdinPersson to learn GAN code. ( I am a learner so might have some mistakes, please let me know so i can correct them and learn better)"
      ],
      "metadata": {
        "id": "Y7GjAvvlH-HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Formula\n",
        "1. GAN consists of mainly discriminators and generators (just names) where each of them are seperate AI models built to correct each other.\n",
        "2. GAN is generally used to generate new information in the form of images or texts. That is why the name, Genrative Adversial Network.\n",
        "3. The job of the genrator model is to generate new information with its best accuracy while the job of thw discriminator is to classify the predicted data as a fake or a real when passed through it while comparing to real training data.\n",
        "4. The optimizer for the generator improves the generator to produce better information or generate better information whereas the job for the optimizer of discriminator is to improve the predictability of the discriminator to classify between the fake and real data.\n",
        "5. Discriminator mainly works on the equation- `log(D(real)) + log(1-D(G(noise)))` where the discriminator tries to max out this equation . So, it tries to reduce the noise and improve the real collected data.\n",
        "6. Generator works on the equation- `log(D(G(noise)))`. Its goal is to minimize this function to produce the best image. If noise is reduced, the model generates better data.\n",
        "\n",
        "##Let us see the working on MNIST dataset"
      ],
      "metadata": {
        "id": "2qDLSK-HDyGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing all necessary libraries and packages"
      ],
      "metadata": {
        "id": "NePWPFE4HODy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ],
      "metadata": {
        "id": "ot0EzW7RHgHB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building Discriminator Model\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,img_dim):\n",
        "    super().__init__()\n",
        "    self.gen=nn.Sequential(\n",
        "        nn.Linear(img_dim,256),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(256,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.gen(x)"
      ],
      "metadata": {
        "id": "JjvhP2ocHnYn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building Generator Model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,img_dim,noise_dim):\n",
        "    super().__init__()\n",
        "    self.gen=nn.Sequential(\n",
        "        nn.Linear(noise_dim,256),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(256,img_dim),\n",
        "        nn.Tanh() #using tanh as it gives the output as a value btw -1 and 1 and we are going to normalise the dataset between -1 and 1.\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.gen(x)"
      ],
      "metadata": {
        "id": "bNtvUJekI9Vb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters (incredbily sensitive to hyperparamters)\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr=3e-4 #allegedly the best value for learning rate of adam optimizer\n",
        "noise_dim=64 #random\n",
        "img_dim=28*28*1 #flat value\n",
        "batch_size=32\n",
        "num_epochs=50"
      ],
      "metadata": {
        "id": "NMsUkiYKJvNw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc=Discriminator(img_dim).to(device)\n",
        "gen=Generator(img_dim,noise_dim).to(device)\n",
        "fixed_noise=torch.randn((batch_size,noise_dim)).to(device) #torch.randn creates random number for the given tensor size witht the mean of 0 and standard deviation of 1\n",
        "transforms=transforms.Compose(\n",
        "    [transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))] #totensor changes the numpy array conversion of the data to a tensor and normalize normalizes the value between wahtever specification is given\n",
        "\n",
        ")\n",
        "dataset=datasets.MNIST(root=\"dataset/\",transform=transforms,download=True)\n",
        "loader=DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
        "opt_disc=optim.Adam(disc.parameters(),lr=lr)\n",
        "opt_gen=optim.Adam(gen.parameters(),lr=lr)\n",
        "criterion=nn.BCELoss() #loss function\n",
        "writer_fake=SummaryWriter(f\"runs/GAN_MNIST/fake\") #stores all generated images\n",
        "writer_real=SummaryWriter(f\"runs/GAN_MNIST/real\") #has all real images\n",
        "step=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMiJqt50KG12",
        "outputId": "33c8be2d-f0ce-4a52-b6e8-8dcfa4d4250a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:07<00:00, 1389121.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 125006.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:03<00:00, 512035.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3224530.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (real,_) in enumerate(loader): #not taking labels means data is unsupervised\n",
        "    real=real.view(-1,784).to(device)\n",
        "    batch_size=real.shape[0]\n",
        "\n",
        "    #training discriminator (maximising (`log(D(real)+log(1-D(G(noise)))`))\n",
        "    noise=torch.randn(batch_size,noise_dim).to(device)\n",
        "    fake=gen(noise)\n",
        "    disc_real=disc(real).view(-1)\n",
        "    lossD_real=criterion(disc_real,torch.ones_like(disc_real)) #maximising d(real) #close to 1\n",
        "    disc_fake=disc(fake).view(-1)\n",
        "    lossD_fake=criterion(disc_fake,torch.zeros_like(disc_fake)) #minimizing d(g(noise)) #close to 0\n",
        "    lossD=(lossD_real+lossD_fake)/2\n",
        "    disc.zero_grad()\n",
        "    lossD.backward(retain_graph=True)#using retain graph helps us to retain the loss value whichwould hve gotten removed after using the backward function t save memory\n",
        "    opt_disc.step()\n",
        "\n",
        "    #training generator (minimizing (`log(1-D(G(noise)))` or max of `D(G(noise))` (to prevent the model to stop optimizing)))\n",
        "    output=disc(fake).view(-1)\n",
        "    lossG=criterion(output,torch.ones_like(output))\n",
        "    gen.zero_grad()\n",
        "    lossG.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "    if batch_idx==0:\n",
        "      print(\n",
        "          f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "          Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
        "      )\n",
        "\n",
        "      with torch.no_grad():\n",
        "        fake=gen(fixed_noise).reshape(-1,1,28,28)\n",
        "        data=real.reshape(-1,1,28,28)\n",
        "        img_grid_fake=torchvision.utils.make_grid(fake,normalize=True)\n",
        "        img_grid_real=torchvision.utils.make_grid(data,normalize=True)\n",
        "        writer_fake.add_image(\"MNIST fake images\",img_grid_fake,global_step=step)\n",
        "        writer_real.add_image(\"MNIST real images\",img_grid_real,global_step=step)\n",
        "        step+=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0P1tMGJM1gU",
        "outputId": "2f6a632c-a7e5-46a1-851e-8f0752df8009"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/50] Batch 0/1875           Loss D: 0.5418, loss G: 0.6971\n",
            "Epoch [1/50] Batch 0/1875           Loss D: 0.4653, loss G: 1.1987\n",
            "Epoch [2/50] Batch 0/1875           Loss D: 0.4055, loss G: 1.2207\n",
            "Epoch [3/50] Batch 0/1875           Loss D: 0.3936, loss G: 1.3665\n",
            "Epoch [4/50] Batch 0/1875           Loss D: 0.4746, loss G: 1.3271\n",
            "Epoch [5/50] Batch 0/1875           Loss D: 0.5546, loss G: 1.0204\n",
            "Epoch [6/50] Batch 0/1875           Loss D: 0.4737, loss G: 1.6329\n",
            "Epoch [7/50] Batch 0/1875           Loss D: 0.5120, loss G: 1.3777\n",
            "Epoch [8/50] Batch 0/1875           Loss D: 0.5833, loss G: 1.4158\n",
            "Epoch [9/50] Batch 0/1875           Loss D: 0.7977, loss G: 1.0708\n",
            "Epoch [10/50] Batch 0/1875           Loss D: 0.5976, loss G: 1.0690\n",
            "Epoch [11/50] Batch 0/1875           Loss D: 0.6568, loss G: 1.2099\n",
            "Epoch [12/50] Batch 0/1875           Loss D: 0.4803, loss G: 1.4783\n",
            "Epoch [13/50] Batch 0/1875           Loss D: 0.5705, loss G: 1.3571\n",
            "Epoch [14/50] Batch 0/1875           Loss D: 0.5575, loss G: 1.7398\n",
            "Epoch [15/50] Batch 0/1875           Loss D: 0.5535, loss G: 1.6459\n",
            "Epoch [16/50] Batch 0/1875           Loss D: 0.5326, loss G: 1.8596\n",
            "Epoch [17/50] Batch 0/1875           Loss D: 0.6053, loss G: 1.7037\n",
            "Epoch [18/50] Batch 0/1875           Loss D: 0.6357, loss G: 1.5445\n",
            "Epoch [19/50] Batch 0/1875           Loss D: 0.4732, loss G: 1.9748\n",
            "Epoch [20/50] Batch 0/1875           Loss D: 0.4077, loss G: 1.5269\n",
            "Epoch [21/50] Batch 0/1875           Loss D: 0.5366, loss G: 1.1349\n",
            "Epoch [22/50] Batch 0/1875           Loss D: 0.5885, loss G: 1.5919\n",
            "Epoch [23/50] Batch 0/1875           Loss D: 0.5014, loss G: 2.0316\n",
            "Epoch [24/50] Batch 0/1875           Loss D: 0.5218, loss G: 1.7767\n",
            "Epoch [25/50] Batch 0/1875           Loss D: 0.4653, loss G: 1.9607\n",
            "Epoch [26/50] Batch 0/1875           Loss D: 0.6039, loss G: 1.1839\n",
            "Epoch [27/50] Batch 0/1875           Loss D: 0.6103, loss G: 1.6533\n",
            "Epoch [28/50] Batch 0/1875           Loss D: 0.5138, loss G: 1.7404\n",
            "Epoch [29/50] Batch 0/1875           Loss D: 0.4753, loss G: 1.4860\n",
            "Epoch [30/50] Batch 0/1875           Loss D: 0.4688, loss G: 2.3835\n",
            "Epoch [31/50] Batch 0/1875           Loss D: 0.5535, loss G: 1.7171\n",
            "Epoch [32/50] Batch 0/1875           Loss D: 0.4421, loss G: 1.5078\n",
            "Epoch [33/50] Batch 0/1875           Loss D: 0.5674, loss G: 1.1745\n",
            "Epoch [34/50] Batch 0/1875           Loss D: 0.4286, loss G: 2.2985\n",
            "Epoch [35/50] Batch 0/1875           Loss D: 0.5271, loss G: 1.7117\n",
            "Epoch [36/50] Batch 0/1875           Loss D: 0.3691, loss G: 1.4661\n",
            "Epoch [37/50] Batch 0/1875           Loss D: 0.4158, loss G: 2.3017\n",
            "Epoch [38/50] Batch 0/1875           Loss D: 0.5387, loss G: 1.3151\n",
            "Epoch [39/50] Batch 0/1875           Loss D: 0.4835, loss G: 1.6130\n",
            "Epoch [40/50] Batch 0/1875           Loss D: 0.5691, loss G: 1.2960\n",
            "Epoch [41/50] Batch 0/1875           Loss D: 0.5526, loss G: 1.0802\n",
            "Epoch [42/50] Batch 0/1875           Loss D: 0.4759, loss G: 1.5013\n",
            "Epoch [43/50] Batch 0/1875           Loss D: 0.3487, loss G: 1.5649\n",
            "Epoch [44/50] Batch 0/1875           Loss D: 0.3898, loss G: 1.4743\n",
            "Epoch [45/50] Batch 0/1875           Loss D: 0.4603, loss G: 1.2413\n",
            "Epoch [46/50] Batch 0/1875           Loss D: 0.4028, loss G: 2.0829\n",
            "Epoch [47/50] Batch 0/1875           Loss D: 0.7385, loss G: 1.1851\n",
            "Epoch [48/50] Batch 0/1875           Loss D: 0.4182, loss G: 1.6639\n",
            "Epoch [49/50] Batch 0/1875           Loss D: 0.4796, loss G: 1.7556\n"
          ]
        }
      ]
    }
  ]
}